# -*- coding: utf-8 -*-
"""ML_Lab_10.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KUG-nilqtwpECx7FJrKmO33K6eia9QXg
"""

"""QUESTION-1
Say you are standing at the bottom of a staircase with  a dice. With each throw of the dice you either move down one 
 step (if you get a 1 or 2 on the dice) or move up one step (if you get a 3, 4 or 5 on the dice). If you throw a 6 on 
 the dice, you throw the dice again and move up the staircase by the number you get on that second throw. Note if you 
 are on the base of the staircase you cannot move down! What is the probability that you will reach more than 60 steps 
 after 250 throws of the dice. Change the code so that you have a function that takes as parameter, the number of throws
Add a new parameter to the function that takes a probability distribution over all outcomes from a dice throw.
For example (0.2,0.3,0.2,0.1,0.1,0.1) would suggest that the probability of getting a 1 is 0.2, 2 is 0.3 etc.
How does that change the probability of reaching a step higher than 60?
"""

import matplotlib.pyplot as plt
from matplotlib import style
import numpy as np
import pandas as pd

import random
dice=0
for i in range(250):
    walk = [0] 
    for x in range(100):
        step = walk[-1] 
        dice = random.randint(1,7)
        if dice <= 2 :
            step = max(0, step - 1)

        elif dice<=5:
            step += 1
        else:
            step = step + random.randint(1,7)
    print(step)

"""QUESTION-2
Generate random data for for Multiple Linear Regression, Logistic Regression, K-mean Clustering
"""

from matplotlib import pyplot
from sklearn.datasets import make_blobs
from sklearn.datasets import make_regression
from pandas import DataFrame

class GenerateData():
  def logisticRegre(self):
    # generate 2d classification dataset
    X2, y2 = make_blobs(n_samples=100, centers=2, n_features=2)
    # scatter plot, dots colored by class value
    df = DataFrame(dict(x=X2[:,0], y=X2[:,1], label=y2))
    colors = {0:'red', 1:'blue'}
    fig, ax = pyplot.subplots()
    grouped = df.groupby('label')
    for key, group in grouped:
        group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])
    pyplot.show()
    return X2, y2
  def linearRegre(self):
    # generate regression dataset
    X1, y1 = make_regression(n_samples=100, n_features=3, noise=0.3)
    # plot regression dataset
    pyplot.scatter(X1[:,0]+X1[:,1]+X1[:,2],y1)
    pyplot.show()
    return X1, y1
  def kMeans(self):
    # generate 2d classification dataset
    X3, y3 = make_blobs(n_samples=100, centers=3, n_features=2)
    # scatter plot, dots colored by class value
    df = DataFrame(dict(x=X3[:,0], y=X3[:,1], label=y3))
    colors = {0:'red', 1:'blue', 2:'green'}
    fig, ax = pyplot.subplots()
    grouped = df.groupby('label')
    for key, group in grouped:
        group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])
    pyplot.show()
    return X3, y3

"""QUESTION-2
Generate random data for for Multiple Linear Regression, Logistic Regression, K-mean Clustering
"""
print("\n-----------------------------------------------------------------------------------------------------------------\nLinear Regression")
import numpy as np
import pandas as pd
import scipy
import random
from scipy.stats import norm
random.seed(1)
n= 3
X=[]
for i in range(0,n):
    X_i= scipy.stats.norm.rvs(0, 1, 100)
    X.append(X_i)
eps=scipy.stats.norm.rvs(0, 1, 100)
y = 1 + (0.5 * X[0]) + (0.4 * X[1]) + (0.3 * X[2])  + eps
data_ols = {'X0': X[0],'X1':X[1],'X2':X[2] ,'Y': y }
df = pd.DataFrame(data_ols)
print(df.head())
print(df.tail())
print(df.info())
print(df.describe())

print("\n-----------------------------------------------------------------------------------------------------------------\nLogistic Regression")

X = []
n = 3
for i in range(0,n):
  X_i = scipy.stats.norm.rvs(0, 1, 100)
  X.append(X_i)
odds = (np.exp(1 + (0.5 * X[0]) + (0.4 * X[1]) + (0.3 * X[2])) /(1 + np.exp(1 + (0.5 * X[0]) + (0.4 * X[1]) + (0.3 * X[2]) ))) 
y1 = [ ]
for i in odds:
  if (i>=0.5):
    y1.append(1)
  else:
    y1.append(0)
data_lr = {'X0': X[0],'X1':X[1],'X2':X[2] ,'Y': y1 }
df1 = pd.DataFrame(data_lr)
print(df.head())
print(df.tail())
print(df.info())
print(df.describe())

print("\n-----------------------------------------------------------------------------------------------------------------\nK-Means Clustering")

X_a= -2 * np.random.rand(100,2)
X_b = 1 + 2 * np.random.rand(50,2)
X_a[50:100, :] = X_b
plt.scatter(X_a[ : , 0], X_a[ :, 1], s = 50)
plt.show()
data_kmeans = {'X0': X_a[:,0],'X1':X_a[:,1]}
df3 = pd.DataFrame(data_kmeans)
print(df.head())
print(df.tail())
print(df.info())
print(df.describe())

"""QUESTION 3
Implement the following algorithms from scratch using numpy only and using the data from Question 1 “for loop”
a. Linear Regression using Gradient Descent
b. Logistic Regression using Gradient Descent
c. Linear Regression with L1 and L2 Regularization
d. Logistic Regression with  L1 and L2 Regularization
e. K-Means
"""
"""
LINEAR REGRESSION WITH GRADIENT DESCENT
"""
print("\n----------------------------------------------------------------------------------------------------------------------------------------------------------------")
print("\n Linear Regression with Gradient Descent")
X = df.iloc[:,0].values
#print(X)
y = df.iloc[:,3].values
b1 = 0
b0 = 0
l = 0.001
epochs = 150
 
n = float(len(X))
for i in range(epochs):
  y_p = b1*X + b0
  loss = np.sum(y_p - y1)**2
  d1 = (-2/n) * sum(X * (y - y_p))
  d0 = (-2/n) * sum(y - y_p)
  b1 = b1 - (l*d1)
  b0 = b0 - (l*d0)

print("B1=",b1," B0=",b0)
print("\n----------------------------------------------------------------------------------------------------------------------------------------------------------------")

"""LOGISTIC REGRESSION WITH GRADIENT DECENT"""
print("Logistic Regression with Gradient Descent")
X1 = df1.iloc[:,0:3].values
y1 = df1.iloc[:,3].values

def sigmoid(Z):
  return 1 /(1+np.exp(-Z))

def loss(y1,y_hat):
  return -np.mean(y1*np.log(y_hat) + (1-y1)*(np.log(1-y_hat)))

W = np.zeros((3,1))
b = np.zeros((1,1))

m = len(y1)
lr = 0.001
for epoch in range(1000):
  Z = np.matmul(X1,W)+b
  A = sigmoid(Z)
  logistic_loss = loss(y1,A)
  dz = A - y1
  dw = 1/m * np.matmul(X1.T,dz)
  db = np.sum(dz)
  W = W - lr*dw
  b = b - lr*db

  if epoch % 100 == 0:
    print("logistic loss :",logistic_loss)
print("\n----------------------------------------------------------------------------------------------------------------------------------------------------------------")

"""LINEAR REGRESSION WITH L1 REGULARIZATION"""
print("Linear Regression with L1 Regularization")
X = df.iloc[:,0].values
y = df.iloc[:,3].values
b1 = 0
b0 = 0
l = 0.001
epochs = 100
lam = 0.1
 
n = float(len(X))
for i in range(epochs):
  y_p = b1*X + b0
  loss = np.sum(y_p - y1)**2 + (lam * b1)
  d1 = (-2/n) * sum(X * (y - y_p)) + lam
  d0 = (-2/n) * sum(y - y_p)
  b1 = b1 - (l*d1)
  b0 = b0 - (l*d0)

print("B1=",b1," B0=",b0)
print("\n----------------------------------------------------------------------------------------------------------------------------------------------------------------")

"""LINEAR REGRESSION WITH L2 REGULARIZATION"""
print("Linear Regression with L2 Regularization")
X = df.iloc[:,0].values
#print(X)
y = df.iloc[:,3].values
b1 = 0
b0 = 0
l = 0.001
epochs = 100
lam = 0.1
 
n = float(len(X))
for i in range(epochs):
  y_p = b1*X + b0
  loss = np.sum(y_p - y1)**2 + ((lam/2) * b1)
  d1 = (-2/n) * sum(X * (y - y_p)) + (lam *b1)
  d0 = (-2/n) * sum(y - y_p)
  b1 = b1 - (l*d1)
  b0 = b0 - (l*d0)

print("B1=",b1," B0=",b0)
print("\n----------------------------------------------------------------------------------------------------------------------------------------------------------------")

"""LOGISTIC REGRESSION WITH L1 REGULARIZATION"""
print("Logistic Regression with L1 Regularization")
X1 = df1.iloc[:,0:3].values
y1 = df1.iloc[:,3].values
lam = 0.1
def sigmoid(Z):
  return 1 /(1+np.exp(-Z))

def loss(y1,y_hat):
  return -np.mean(y1*np.log(y_hat) + (1-y1)*(np.log(1-y_hat))) + (lam * (np.sum(W)))

W = np.zeros((3,1))
b = np.zeros((1,1))

m = len(y1)
lr = 0.001
for epoch in range(1000):
  Z = np.matmul(X1,W)+b
  A = sigmoid(Z)
  logistic_loss = loss(y1,A)
  dz = A - y1
  dw = 1/m * np.matmul(X1.T,dz) + lam
  db = np.sum(dz)

  W = W - lr*dw
  b = b - lr*db

  if epoch % 100 == 0:
    print("logistic loss :",logistic_loss)
print("\n----------------------------------------------------------------------------------------------------------------------------------------------------------------")

"""LOGISTIC REGRESSION WITH L2 REGULARIZATION"""
print("Logistic Regression with L2 Regularization")
X1 = df1.iloc[:,0:3].values
y1 = df1.iloc[:,3].values
lam = 0.1
def sigmoid(Z):
  return 1 /(1+np.exp(-Z))

def loss(y1,y_hat):
  return -np.mean(y1*np.log(y_hat) + (1-y1)*(np.log(1-y_hat))) + (lam * (np.sum(np.square(W))))

W = np.zeros((3,1))
b = np.zeros((1,1))

m = len(y1)
lr = 0.001
for epoch in range(1000):
  Z = np.matmul(X1,W)+b
  A = sigmoid(Z)
  logistic_loss = loss(y1,A)
  dz = A - y1
  dw = 1/m * np.matmul(X1.T,dz) + lam * W
  db = np.sum(dz)

  W = W - lr*dw
  b = b - lr*db

  if epoch % 100 == 0:
    print("logistic loss :",logistic_loss)
print("\n----------------------------------------------------------------------------------------------------------------------------------------------------------------")

"""K-MEANS CLUSTERING ALGORITHM"""
print("K-Means Clustering")
class K_Means:
    def __init__(self, k=2, tol=0.001, max_iter=300):
        self.k = k
        self.tol = tol
        self.max_iter = max_iter

    def fit(self,data):

        self.centroids = {}

        for i in range(self.k):
            self.centroids[i] = data[i]

        for i in range(self.max_iter):
            self.classifications = {}

            for i in range(self.k):
                self.classifications[i] = []

            for featureset in X:
                distances = [np.linalg.norm(featureset-self.centroids[centroid]) for centroid in self.centroids]
                classification = distances.index(min(distances))
                self.classifications[classification].append(featureset)

            prev_centroids = dict(self.centroids)

            for classification in self.classifications:
                self.centroids[classification] = np.average(self.classifications[classification],axis=0)

            optimized = True

            for c in self.centroids:
                original_centroid = prev_centroids[c]
                current_centroid = self.centroids[c]
                if np.sum((current_centroid-original_centroid)/original_centroid*100.0) > self.tol:
                    print(np.sum((current_centroid-original_centroid)/original_centroid*100.0))
                    optimized = False

            if optimized:
                break

    def predict(self,data):
        distances = [np.linalg.norm(data-self.centroids[centroid]) for centroid in self.centroids]
        classification = distances.index(min(distances))
        return classification
        
colors = 10*["g","r","c","b","k"]

X = df3.iloc[:,0:2].values
clf = K_Means()
clf.fit(X)

for centroid in clf.centroids:
    plt.scatter(clf.centroids[centroid][0], clf.centroids[centroid][1],
                marker="o", color="k", s=150, linewidths=5)

for classification in clf.classifications:
    color = colors[classification]
    for featureset in clf.classifications[classification]:
        plt.scatter(featureset[0], featureset[1], marker="x", color=color, s=150, linewidths=5)

print("\nLogistic Regression Using Oops")
from sklearn.model_selection import train_test_split
class LogisticRegression:
    def __init__(self, lr=0.01, num_iter=100000, fit_intercept=True):
        self.lr = lr
        self.num_iter = num_iter
        self.fit_intercept = fit_intercept
    
    def __add_intercept(self, X):
        intercept = np.ones((X.shape[0], 1))
        return np.concatenate((intercept, X), axis=1)
    
    def __sigmoid(self, z):
        return 1 / (1 + np.exp(-z))

    def __loss(self, h, y):
        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()
    
    def fit(self, X, y):
        if self.fit_intercept:
            X = self.__add_intercept(X)
        
        # weights initialization
        self.theta = np.zeros(X.shape[1])
        
        for i in range(self.num_iter):
            z = np.dot(X, self.theta)
            h = self.__sigmoid(z)
            gradient = np.dot(X.T, (h - y)) / y.size
            self.theta -= self.lr * gradient
    
    def predict_prob(self, X):
        if self.fit_intercept:
            X = self.__add_intercept(X)
    
        return self.__sigmoid(np.dot(X, self.theta))
    
    def predict(self, X, threshold):
        return self.predict_prob(X) >= threshold
    def plot(self):
        plt.figure(figsize=(10, 6))
        plt.scatter(X[y == 0][:, 0], X[y == 0][:, 1], color='b', label='0')
        plt.scatter(X[y == 1][:, 0], X[y == 1][:, 1], color='r', label='1')
        plt.legend()
        x1_min, x1_max = X[:,0].min(), X[:,0].max(),
        x2_min, x2_max = X[:,1].min(), X[:,1].max(),
        xx1, xx2 = np.meshgrid(np.linspace(x1_min, x1_max), np.linspace(x2_min, x2_max))
        grid = np.c_[xx1.ravel(), xx2.ravel()]
        probs = model.predict_prob(grid).reshape(xx1.shape)
        plt.contour(xx1, xx2, probs, [0.5], linewidths=1, colors='black');
gd = GenerateData()
X, y = gd.logisticRegre()
Y = y.reshape(-1,1)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
model = LogisticRegression()
model.fit(X_train, y_train)
model.plot()

print("\n---------------------------------------------------------------------------------------------------------------------------------------\nLinear Regression Using Oops")
class LinearRegression():
    def __init__(self, X, y, alpha=0.03, n_iter=1500):

        self.alpha = alpha
        self.n_iter = n_iter
        self.n_samples = len(y)
        self.n_features = np.size(X, 1)
        self.X = np.hstack((np.ones(
            (self.n_samples, 1)), (X - np.mean(X, 0)) / np.std(X, 0)))
        self.y = y[:, np.newaxis]
        self.params = np.zeros((self.n_features + 1, 1))
        self.coef_ = None
        self.intercept_ = None

    def fit(self):

        for i in range(self.n_iter):
            self.params = self.params - (self.alpha/self.n_samples) * \
            self.X.T @ (self.X @ self.params - self.y)

        self.intercept_ = self.params[0]
        self.coef_ = self.params[1:]

        return self

    def score(self, X=None, y=None):

        if X is None:
            X = self.X
        else:
            n_samples = np.size(X, 0)
            X = np.hstack((np.ones(
                (n_samples, 1)), (X - np.mean(X, 0)) / np.std(X, 0)))

        if y is None:
            y = self.y
        else:
            y = y[:, np.newaxis]

        y_pred = X @ self.params
        score = 1 - (((y - y_pred)**2).sum() / ((y - y.mean())**2).sum())

        return score

    def predict(self, X):
        n_samples = np.size(X, 0)
        y = np.hstack((np.ones((n_samples, 1)), (X-np.mean(X, 0)) \
                            / np.std(X, 0))) @ self.params
        return y

    def get_params(self):
        return self.params

from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np

gd = GenerateData()
X, y = gd.linearRegre()

X_train, X_test, y_train, y_test = train_test_split(\
                X, y, test_size=0.3, random_state=42)

our_regressor = LinearRegression(X_train, y_train).fit()

our_train_accuracy = our_regressor.score()

our_test_accuracy = our_regressor.score(X_test, y_test)

pd.DataFrame([[our_train_accuracy],
              [our_test_accuracy]],
             ['Training Accuracy', 'Test Accuracy'])

print("\n---------------------------------------------------------------------------------------------------------------------------------------\nLinear Regression Using Oops")
class K_Means:
    def __init__(self, k=2, tol=0.001, max_iter=300):
        self.k = k
        self.tol = tol
        self.max_iter = max_iter

    def fit(self,data):

        self.centroids = {}

        for i in range(self.k):
            self.centroids[i] = data[i]

        for i in range(self.max_iter):
            self.classifications = {}

            for i in range(self.k):
                self.classifications[i] = []

            for featureset in data:
                distances = [np.linalg.norm(featureset-self.centroids[centroid]) for centroid in self.centroids]
                classification = distances.index(min(distances))
                self.classifications[classification].append(featureset)

            prev_centroids = dict(self.centroids)

            for classification in self.classifications:
                self.centroids[classification] = np.average(self.classifications[classification],axis=0)

            optimized = True

            for c in self.centroids:
                original_centroid = prev_centroids[c]
                current_centroid = self.centroids[c]
                if np.sum((current_centroid-original_centroid)/original_centroid*100.0) > self.tol:
                    print(np.sum((current_centroid-original_centroid)/original_centroid*100.0))
                    optimized = False

            if optimized:
                break

    def predict(self,data):
        distances = [np.linalg.norm(data-self.centroids[centroid]) for centroid in self.centroids]
        classification = distances.index(min(distances))
        return classification

gd = GenerateData()
X, y = gd.kMeans()
clf = K_Means()
clf.fit(X)

